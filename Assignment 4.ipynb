{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarquin Bennett "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  [[0.23911517 0.08621825 0.03512189]]\n",
      "Accuracy:  0.7627986348122867\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv(\"RFMdataMPJ.csv\", header=0)\n",
    "\n",
    "X= data.iloc[:586, :-1]\n",
    "y= data.iloc[:586, -1]\n",
    "#had to set the endpoint because there are random null characters that the program took as NaN\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy = accuracy_score(y,predicted_classes)\n",
    "parameters = model.coef_\n",
    "\n",
    "print(\"Parameters: \",parameters)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters output tells us which of the parameters are most useful in guessing the response. Accuracy is how accurate the guess from the model would be. To increase the accuracy we would need to have more data to train the model. This would also give us a better idea of which parameters effect the response. However at 76% accuracy, the improvements would not that large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \n",
      " [[-0.24494715  0.19204641  0.18755669]\n",
      " [-0.18708326  0.24587906  0.12176953]\n",
      " [ 0.04886964 -0.02253952  0.59184048]\n",
      " [ 0.2060289  -0.12692129 -0.7877625 ]\n",
      " [ 0.17713188 -0.28846466 -0.11340421]]\n",
      "Accuracy:  0.34\n",
      "Confusion Matrix: \n",
      " [[ 4 11  3  2 14]\n",
      " [ 2 12  1  1 14]\n",
      " [ 0  5  5  3 29]\n",
      " [ 0  8  2  0 27]\n",
      " [ 1  3  2  4 47]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "products = pd.read_csv(\"./amazon_review_full_csv/train.csv\")\n",
    "testProducts = pd.read_csv('./amazon_review_full_csv/test.csv')\n",
    "\n",
    "for i in range(0,len(products)-1):\n",
    "    if type(products.iloc[i]['review']) != str:\n",
    "        products.iloc[i]['review'] = str(products.iloc[i]['review'])\n",
    "\n",
    "for i in range(0,len(testProducts)-1):\n",
    "    if type(testProducts.iloc[i]['review']) != str:\n",
    "        testProducts.iloc[i]['review'] = str(testProducts.iloc[i]['review'])\n",
    "        \n",
    "negators=[\"No\",\"Not\",\"None\",\"Nobody\",\"Nothing\",\"Neither\",\"Nowhere\",\"Never\",\"Hardly\",\"Scarcely\",\"Barely\",\"Doesn’t\",\"Isn’t\",\"Wasn’t\",\"Shouldn’t\",\"Wouldn’t\",\"Couldn’t\",\"Won’t\",\"Can’t\",\"Don’t\"]\n",
    "\n",
    "posfile = open(\"positive-words.txt\", 'r')\n",
    "positive = [line.split() for line in posfile.readlines()]\n",
    "posfile.close()\n",
    "positive = flatten(positive[positive.index([])+1:])\n",
    "\n",
    "negfile = open(\"negative-words.txt\",'r')\n",
    "negative = [line.split() for line in negfile.readlines()]\n",
    "negfile.close()\n",
    "negative = flatten(negative[negative.index([])+1:])\n",
    "\n",
    "# def combined_features(row):\n",
    "#     return row['title'] + ' '+ row['review']\n",
    "# products['all_features'] = products.apply(combined_features, axis=1)\n",
    "\n",
    "reviewCount=[]\n",
    "rating=[]\n",
    "i=0\n",
    "for review in products['review']:\n",
    "    pcount=0\n",
    "    ncount=0\n",
    "    negcount=0\n",
    "    for word in review.split():\n",
    "        if word in positive:\n",
    "            pcount+=1\n",
    "        elif word in negative:\n",
    "            ncount+=1\n",
    "        elif word in negators:\n",
    "            negcount+=1\n",
    "    reviewCount.append([pcount,ncount,negcount])\n",
    "    rating.append(products['rating'][i])\n",
    "    i+=1\n",
    "  \n",
    "testReviewCount=[]\n",
    "testRating=[]\n",
    "i=0\n",
    "for review in testProducts['review']:\n",
    "    pcount=0\n",
    "    ncount=0\n",
    "    negcount=0\n",
    "    for word in review.split():\n",
    "        if word in positive:\n",
    "            pcount+=1\n",
    "        elif word in negative:\n",
    "            ncount+=1\n",
    "        elif word in negators:\n",
    "            negcount+=1\n",
    "    testReviewCount.append([pcount,ncount,negcount])\n",
    "    testRating.append(testProducts['rating'][i])\n",
    "    i+=1\n",
    "\n",
    "X=reviewCount\n",
    "y=rating\n",
    "xTest=testReviewCount\n",
    "yTest=testRating\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "predicted_classes = model.predict(xTest)\n",
    "accuracy = model.score(xTest,yTest)\n",
    "parameters = model.coef_\n",
    "\n",
    "print(\"Parameters: \\n\",parameters)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "\n",
    "cm= confusion_matrix(yTest,predicted_classes)\n",
    "print(\"Confusion Matrix: \\n\",cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of our model is 34% which is not very good. This could be from the lack of variety in the dataset and/or the lack of data. From the parameters we see that there are one that really has an effect on the model and a lot of parameters that have no effect. The confusion matrix tells us on what parts the model did good on. We can see that the model did good on the last column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=23, size=100, alpha=0.025)\n",
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final', 'hello', 'world', 'test', 'number', 'two', 'predictions', 'are', 'being', 'made']\n",
      "[ 2.5227522e-03 -6.2937848e-04 -4.9029109e-03 -2.4396258e-03\n",
      "  2.3287174e-03  1.0548813e-03  1.6651042e-03  5.4571792e-05\n",
      " -1.9733252e-03 -2.4534883e-03 -2.0933445e-03  4.3389732e-03\n",
      " -1.2930693e-03 -1.9221391e-03  6.6902075e-04  9.1030565e-04\n",
      " -3.1228594e-03 -3.7955772e-03 -3.6938242e-03  2.1259966e-03\n",
      "  3.5127609e-03 -4.7598728e-03  2.1459716e-03  4.4613602e-03\n",
      "  3.5343145e-03 -3.1231414e-03  2.8843095e-03 -6.5252511e-04\n",
      " -7.2102062e-04 -7.6481077e-04  2.2518656e-03 -3.3790446e-03\n",
      "  7.9832214e-04 -6.5784756e-04 -2.6390592e-03 -6.7077321e-04\n",
      "  4.9757264e-03 -2.5091625e-03 -4.0667583e-03  7.1139616e-04\n",
      " -2.2016396e-03  2.5321422e-03 -1.6454428e-03  4.1974606e-03\n",
      " -3.7643609e-03  1.2072896e-03  7.1447960e-04 -3.5500433e-03\n",
      "  2.4229540e-03  2.7221313e-03 -4.4860947e-03 -3.4785422e-03\n",
      "  2.1015145e-03 -2.9547917e-04 -7.6350849e-04  3.6495687e-03\n",
      "  6.4606598e-04 -4.7443970e-03  3.7836623e-03 -1.3731598e-03\n",
      "  1.9107379e-03 -1.7278380e-03  4.0938118e-03 -4.5430474e-03\n",
      " -8.7838431e-05  1.9214508e-03  4.4151712e-03  2.4345578e-03\n",
      " -4.7385329e-03 -1.6242297e-03 -1.4117714e-04 -3.3558283e-03\n",
      "  2.4956632e-03 -2.0614245e-04  4.9888161e-03  3.2408920e-03\n",
      " -3.6980754e-03  6.0311914e-04  1.8652931e-03 -3.7031106e-03\n",
      " -1.7459105e-03  1.2960478e-04 -2.5314908e-03  2.9998724e-03\n",
      " -4.0841936e-03 -4.8536286e-03 -2.6028799e-03 -5.5770436e-04\n",
      "  4.6779821e-03  3.7445875e-03  1.9138168e-03  2.6786297e-03\n",
      " -3.2352237e-03  1.3868485e-03 -3.2539768e-03  1.3034491e-04\n",
      "  1.0423453e-03  4.1618496e-03 -2.9752697e-03  1.1181920e-03]\n",
      "-0.036380887\n",
      "0.0001858133\n",
      "0.05604971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "# define training data\n",
    "\n",
    "trainSentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "            ['this', 'is', 'the', 'second', 'sentence'],\n",
    "            ['yet', 'another', 'sentence'],\n",
    "            ['one', 'more', 'sentence'],\n",
    "            ['and', 'the', 'final', 'sentence'],\n",
    "            ['hello', 'world'],\n",
    "            ['test','sentence','number', 'two'],\n",
    "            ['predictions','are','being','made']]\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(trainSentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "print(model['this'])\n",
    "print(model.similarity('hello','sentence'))\n",
    "print(model.similarity('one','two'))\n",
    "print(model.similarity('is','the'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec takes words into a model and converts them into vectors. With that data we can see similarity of words with respect to the model. Therefore the more you train the model the better the similarity ratings between words will get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by vectorizing documents we can get a better understanding of what the document is saying because it becomes a collection of word vectors. When it comes to sentence tokenizers, by determing the heaviest weighted words in the sentence. BLK uses parts of speech tagging to determine sentences, since every sentence needs to have at least a noun followed by a verb followed by another noun. This works till you get to more complex sentences, which is where document embedding could come in handy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
