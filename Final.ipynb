{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarquin Bennett"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.10.10 Train a unigram tagger and run it on some new text. Observe that some words are not assigned a tag. Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7978585481282695\n",
      "[('On', 'IN'), ('one', 'CD'), ('of', 'IN'), ('his', 'PP$'), ('1921', 'CD'), ('ventures', None), ('he', 'PPS'), ('was', 'BEDZ'), ('actually', 'RB'), ('come', 'VB'), ('upon', 'IN'), ('by', 'IN'), ('a', 'AT'), ('Detective', 'NN-TL'), ('Sergeant', None), ('John', 'NP'), ('J.', 'NP'), ('Ryan', None), ('down', 'RP'), ('on', 'IN'), ('his', 'PP$'), ('knees', 'NNS'), ('with', 'IN'), ('a', 'AT'), ('tool', 'NN'), ('embedded', 'VBN'), ('in', 'IN'), ('a', 'AT'), ('labour', None), ('office', 'NN'), ('safe', 'JJ'), ('in', 'IN'), ('the', 'AT'), ('Postal', None), ('Telegraph', None), ('Building', 'NN-TL'), (';', '.'), (';', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_news_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_lore_tagged_sents = brown.tagged_sents(categories='lore')\n",
    "\n",
    "brown_news_sents = brown.sents(categories='news')\n",
    "brown_lore_sents = brown.sents(categories='lore')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(brown_news_tagged_sents)\n",
    "print(unigram_tagger.evaluate(brown_lore_tagged_sents))\n",
    "print(unigram_tagger.tag(brown_lore_sents[2007]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem uses nltk and the news section of the brown corpus to train an unigram tagger. We then use a different section of the corpus, lore, to then evaluate the tagger and then tag a random sentence. Even with 80% accuracy, we see in the chosen sentence that some 'None' shows up. This is due to the unigram tagger only working with words it has seen. Therefore, if you use the tagger on a sentence not in the training set there is a chance that it comes across words it has not seen. To fix this you would want to have a bigger training set. However this would not completely fix the issue, unless the tagger has every word with each possible tag which would take a long time to train and to tag due to how big the model would be, they will also be instances of words where the model will not know the tag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
