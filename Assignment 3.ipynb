{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarquin Bennett                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 \n",
    "cost of sub is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "/ # b r i e f\n",
    "#[0,1,2,3,4,5]\n",
    "d[1,1,2,3,4,5]\n",
    "r[2,2,1,2,3,4]\n",
    "i[3,3,2,1,2,3]\n",
    "v[4,4,3,2,2,3]\n",
    "e[5,5,4,3,2,3]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    " /# d r i v e r s\n",
    "#[0,1,2,3,4,5,6,7]\n",
    "d[1,0,1,2,3,4,5,6]\n",
    "r[2,1,0,1,2,3,4,5]\n",
    "i[3,2,1,0,1,2,3,4]\n",
    "v[4,3,2,1,0,1,2,3]\n",
    "e[5,4,3,2,1,0,1,2]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit distance for brief to drive is 3 while drive to drivers is 2. Which means drive is closer in edit distance to drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistance(n: str,m: str) -> int:\n",
    "    lenN=len(n)\n",
    "    lenM=len(m)\n",
    "\n",
    "    D=[[0 for _ in range(lenM+1)] for _ in range(lenN+1)]\n",
    "\n",
    "    for i in range(1,lenN+1):\n",
    "        D[i][0] = D[i-1][0]+1\n",
    "\n",
    "    for i in range(1,lenM+1):\n",
    "        D[0][i] = D[0][i-1]+1\n",
    "\n",
    "    for i in range(1,lenN+1):\n",
    "        for j in range(1,lenM+1):\n",
    "            if n[i-1]==m[j-1]:\n",
    "                D[i][j]=D[i-1][j-1]\n",
    "            else:\n",
    "                D[i][j] = min(D[i-1][j]+1,D[i-1][j-1]+1,D[i][j-1]+1)\n",
    "    #prints the grid\n",
    "    for i in D:\n",
    "        print(i)\n",
    "    return D[lenN][lenM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5]\n",
      "[1, 1, 2, 3, 4, 5]\n",
      "[2, 2, 1, 2, 3, 4]\n",
      "[3, 3, 2, 1, 2, 3]\n",
      "[4, 4, 3, 2, 2, 3]\n",
      "[5, 5, 4, 3, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editDistance(\"drive\",\"brief\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[1, 0, 1, 2, 3, 4, 5, 6]\n",
      "[2, 1, 0, 1, 2, 3, 4, 5]\n",
      "[3, 2, 1, 0, 1, 2, 3, 4]\n",
      "[4, 3, 2, 1, 0, 1, 2, 3]\n",
      "[5, 4, 3, 2, 1, 0, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editDistance(\"drive\",\"drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84\n",
      "Most Informative Features\n",
      "        contains(seagal) = True              neg : pos    =     11.0 : 1.0\n",
      "   contains(outstanding) = True              pos : neg    =     11.0 : 1.0\n",
      "         contains(mulan) = True              pos : neg    =      8.3 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      7.6 : 1.0\n",
      "         contains(damon) = True              pos : neg    =      6.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d),c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us the model we trained has a 87% accuracy in correctly classifying positive or negative reviews. We also see the top 5 words that tell us the most information on whether the review is positive or negative. So this means that if we take this model and apply it to undecided data set, there is 87% accuracy that the model will be correct in prediction of positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in \"Reviews Classification Using SentiWordNet Lexicon\", when using SentiWordNet you can classify reviews as positive and negative. This research paper talks about a previous method term counting, and compares it to two new methods that the researchers came up with, sum on review and average on sentence and Average on Review. Term counting is when each word is assigned a positive class or negative class and then counted up and the review is classified base on which class has the higher count. In their research it was accurate 56.77% of the time. Sum on review is when the summation of positive and negative scores for each term is found in a review, is calculated to get the positive and negative scores for all review words. Then based on which score is higher, the review is given that sentiment. This is different from term counting because it takes into account the magnitude scores for the words. This research saw the accuracy of 67% for this method. Average on sentence and Average on Review is when the average sentiment is found for each sentence and then the average sentiment is found for all sentence to get the review its sentiment. The accuracy for this method is 68.63%. Therefore using SentiWordNet with the average on sentence and average on review method, along with Python and WordNet, you can assign a sentiment for a review with a 68.63% accuracy, which is about 10% higher then term counting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test: 0.8267898383371824\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import senseval\n",
    "instances = senseval.instances('hard.pos')\n",
    "size = int(len(instances) * 0.1)\n",
    "\n",
    "def features(instance):\n",
    "    feat = dict()\n",
    "    p = instance.position\n",
    "       ## previous word and tag\n",
    "    if p: ## > 0\n",
    "        feat['wp'] = instance.context[p-1][0]\n",
    "        feat['tp'] = instance.context[p-1][1]\n",
    "       ## use BOS if it is the first word\n",
    "    else: # \n",
    "        feat['wp'] = (p, 'BOS')\n",
    "        feat['tp'] = (p, 'BOS')\n",
    "       ## following word and tag       \n",
    "        feat['wf'] = instance.context[p+1][0]\n",
    "        feat['tf'] = instance.context[p+1][1]\n",
    "    return feat\n",
    "\n",
    "\n",
    "featureset =[(features(i), i.senses[0]) for i in \n",
    "             instances if len(i.senses)==1]\n",
    "\n",
    "random.shuffle(featureset)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (\"Accuracy on Test:\", nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This naive bayes classifier predicts the correct sense tag for a given instance with an accuracy of 82.68%. This allows us to be confidante with the model to apply it to a larger data set and have us get the current sense tag. To improve the accuracy, we would need to train the model on a bigger dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
